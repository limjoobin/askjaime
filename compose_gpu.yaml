services:
  llamacpp-server:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: all
                capabilities: [gpu]